data=learn[,c(selected,TRUE)])
# Calculate CV'd probabilities of being in maturity group 2
cvprobas[segments==k] = predict(image.logit,newdata=test,type="response")
}
cvprobas = rep(0,n) # will contain the cross-validated probabilities
for (k in 1:10) {          # cycling over the 10 segments
# the kth segment is excluded from the learning sample
learn = bact[segments!=k,-7]
# the test sample is just the kth segment
test = bact[segments==k,-7]
# Fit the whole classification rule on the learning sample
image.select = bestglm(Xy=learn,family=binomial,
method="exhaustive",IC="AIC")
# Extract AIC for best models with k explanatory variables
AIC = image.select$Subsets$AIC
# Identifies column numbers for selected explanatory variables
selected = unlist(image.select$Subsets[which.min(AIC),2:6])
# Fit the model with selected axplanatory variables
image.logit = glm(E~.,family=binomial,
data=learn[,c(selected,TRUE)])
# Calculate CV'd probabilities of being in maturity group 2
cvprobas[segments==k] = predict(image.logit,newdata=test,type="response")
}
# Step 2: cycling over the segments
cvprobas = rep(0,n) # will contain the cross-validated probabilities
for (k in 1:10) {          # cycling over the 10 segments
# the kth segment is excluded from the learning sample
learn = bact[segments!=k,-1]
# the test sample is just the kth segment
test = bact[segments==k,-1]
# Fit the whole classification rule on the learning sample
image.select = bestglm(Xy=learn,family=binomial,
method="exhaustive",IC="AIC")
# Extract AIC for best models with k explanatory variables
AIC = image.select$Subsets$AIC
# Identifies column numbers for selected explanatory variables
selected = unlist(image.select$Subsets[which.min(AIC),2:6])
# Fit the model with selected axplanatory variables
image.logit = glm(E~.,family=binomial,
data=learn[,c(selected,TRUE)])
# Calculate CV'd probabilities of being in maturity group 2
cvprobas[segments==k] = predict(image.logit,newdata=test,type="response")
}
cvprobas = rep(0,n) # will contain the cross-validated probabilities
for (k in 1:10) {          # cycling over the 10 segments
# the kth segment is excluded from the learning sample
learn = bact[segments!=k,-2]
# the test sample is just the kth segment
test = bact[segments==k,-2]
# Fit the whole classification rule on the learning sample
image.select = bestglm(Xy=learn,family=binomial,
method="exhaustive",IC="AIC")
# Extract AIC for best models with k explanatory variables
AIC = image.select$Subsets$AIC
# Identifies column numbers for selected explanatory variables
selected = unlist(image.select$Subsets[which.min(AIC),2:6])
# Fit the model with selected axplanatory variables
image.logit = glm(E~.,family=binomial,
data=learn[,c(selected,TRUE)])
# Calculate CV'd probabilities of being in maturity group 2
cvprobas[segments==k] = predict(image.logit,newdata=test,type="response")
}
cvprobas = rep(0,n) # will contain the cross-validated probabilities
for (k in 1:10) {          # cycling over the 10 segments
# the kth segment is excluded from the learning sample
learn = bact[segments!=k,-7]
# the test sample is just the kth segment
test = bact[segments==k,-7]
# Fit the whole classification rule on the learning sample
image.select = bestglm(Xy=learn,family=binomial,
method="exhaustive",IC="AIC")
# Extract AIC for best models with k explanatory variables
AIC = image.select$Subsets$AIC
# Identifies column numbers for selected explanatory variables
selected = unlist(image.select$Subsets[which.min(AIC),2:6])
# Fit the model with selected axplanatory variables
image.logit = glm(E~.,family=binomial,
data=learn[,c(selected,TRUE)])
# Calculate CV'd probabilities of being in maturity group 2
cvprobas[segments==k] = predict(image.logit,newdata=test,type="response")
}
# Step 1: segmentation of the dataset in 10 segments
segments = fold(Data_Maxine_MICHAUX,k=10,cat_col="E")$".folds"
# Step 2: cycling over the segments
cvpred = rep("1",nrow(Data_Maxine_MICHAUX)) # will contain the cross-validated probabilities
for (k in 1:10) {          # cycling over the 10 segments
# the kth segment is excluded from the learning sample
learn = Data_Maxine_MICHAUX[segments!=k,]
# the test sample is just the kth segment
test = Data_Maxine_MICHAUX[segments==k,]
# Fit the model with selected axplanatory variables
bact.mlog = multinom(E~.,
data=Data_Maxine_MICHAUX,trace=FALSE)
bact.select = stepwise(bact.mlog,direction="forward/backward",criterion="AIC")
# Calculate CV'd probabilities of being in maturity group 2
cvpred[segments==k] = predict(bact.select,newdata=test,type="class")
}
# Step 3: Cross-validated performance criteria
b.accurracy = mean(cvpred==bact$E)
b.accuracy
b.accurracy = mean(cvpred==Data_Maxine_MICHAUX$E)
b.accuracy
# Step 1: segmentation of the dataset in 10 segments
segments = fold(Data_Maxine_MICHAUX,k=10,cat_col="E")$".folds"
# 10-fold balanced partition of the sample
# segments is a vector giving the segment number of each item
table(Data_Maxine_MICHAUX$E,segments)
# Step 2: cycling over the segments
cvprobas = rep(0,n) # will contain the cross-validated probabilities
for (k in 1:10) {          # cycling over the 10 segments
# the kth segment is excluded from the learning sample
learn = bact[segments!=k,]
# the test sample is just the kth segment
test = bact[segments==k,]
# Fit the whole classification rule on the learning sample
image.select = bestglm(Xy=learn,family=binomial,
method="exhaustive",IC="AIC")
# Extract AIC for best models with k explanatory variables
AIC = image.select$Subsets$AIC
# Identifies column numbers for selected explanatory variables
selected = unlist(image.select$Subsets[which.min(AIC),2:6])
# Fit the model with selected axplanatory variables
image.logit = glm(E~.,family=binomial,
data=learn[,c(selected,TRUE)])
# Calculate CV'd probabilities of being in maturity group 2
cvprobas[segments==k] = predict(image.logit,newdata=test,type="response")
}
# Step 1: segmentation of the dataset in 10 segments
segments = fold(Data_Maxine_MICHAUX,k=10,cat_col="E")$".folds"
# 10-fold balanced partition of the sample
# segments is a vector giving the segment number of each item
table(Data_Maxine_MICHAUX$E,segments)
# Step 2: cycling over the segments
cvprobas = rep(0,n) # will contain the cross-validated probabilities
for (k in 1:10) {          # cycling over the 10 segments
# the kth segment is excluded from the learning sample
learn = Data_Maxine_MICHAUX[segments!=k,]
# the test sample is just the kth segment
test = Data_Maxine_MICHAUX[segments==k,]
# Fit the whole classification rule on the learning sample
image.select = bestglm(Xy=learn,family=binomial,
method="exhaustive",IC="AIC")
# Extract AIC for best models with k explanatory variables
AIC = image.select$Subsets$AIC
# Identifies column numbers for selected explanatory variables
selected = unlist(image.select$Subsets[which.min(AIC),2:6])
# Fit the model with selected axplanatory variables
image.logit = glm(E~.,family=binomial,
data=learn[,c(selected,TRUE)])
# Calculate CV'd probabilities of being in maturity group 2
cvprobas[segments==k] = predict(image.logit,newdata=test,type="response")
}
View(Data_Maxine_MICHAUX)
image.logit = glm(E~., data=bact, family=binomial)
image.logit = glm(E~., data=Data_Maxine_MICHAUX, family=binomial)
image.logit = glm(E~., data=bact, family=binomial)
# Step 1: segmentation of the dataset in 10 segments
segments = fold(Data_Maxine_MICHAUX,k=10,cat_col="E")$".folds"
# 10-fold balanced partition of the sample
# segments is a vector giving the segment number of each item
table(Data_Maxine_MICHAUX$E,segments)
# Step 2: cycling over the segments
cvprobas = rep(0,n) # will contain the cross-validated probabilities
for (k in 1:10) {          # cycling over the 10 segments
# the kth segment is excluded from the learning sample
learn = Data_Maxine_MICHAUX[segments!=k,]
# the test sample is just the kth segment
test = Data_Maxine_MICHAUX[segments==k,]
# Fit the whole classification rule on the learning sample
image.select = multinom(Xy=learn,family=binomial,
method="exhaustive",IC="AIC")
# Extract AIC for best models with k explanatory variables
AIC = image.select$Subsets$AIC
# Identifies column numbers for selected explanatory variables
selected = unlist(image.select$Subsets[which.min(AIC),2:6])
# Fit the model with selected axplanatory variables
image.logit = glm(E~.,family=binomial,
data=learn[,c(selected,TRUE)])
# Calculate CV'd probabilities of being in maturity group 2
cvprobas[segments==k] = predict(image.logit,newdata=test,type="response")
}
# Step 1: segmentation of the dataset in 10 segments
segments = fold(Data_Maxine_MICHAUX,k=10,cat_col="E")$".folds"
# 10-fold balanced partition of the sample
# segments is a vector giving the segment number of each item
table(Data_Maxine_MICHAUX$E,segments)
# Step 2: cycling over the segments
cvprobas = rep(0,n) # will contain the cross-validated probabilities
for (k in 1:10) {          # cycling over the 10 segments
# the kth segment is excluded from the learning sample
learn = Data_Maxine_MICHAUX[segments!=k,]
# the test sample is just the kth segment
test = Data_Maxine_MICHAUX[segments==k,]
# Fit the whole classification rule on the learning sample
bact.mlog = multinom(E~.,
data=Data_Maxine_MICHAUX,trace=FALSE)
# Extract AIC for best models with k explanatory variables
AIC = image.select$Subsets$AIC
# Identifies column numbers for selected explanatory variables
selected = unlist(image.select$Subsets[which.min(AIC),2:6])
# Fit the model with selected axplanatory variables
image.logit = glm(E~.,family=binomial,
data=learn[,c(selected,TRUE)])
# Calculate CV'd probabilities of being in maturity group 2
cvprobas[segments==k] = predict(image.logit,newdata=test,type="response")
}
# Step 1: segmentation of the dataset in 10 segments
segments = fold(Data_Maxine_MICHAUX,k=10,cat_col="E")$".folds"
# 10-fold balanced partition of the sample
# segments is a vector giving the segment number of each item
table(Data_Maxine_MICHAUX$E,segments)
# Step 2: cycling over the segments
cvprobas = rep(0,n) # will contain the cross-validated probabilities
for (k in 1:10) {          # cycling over the 10 segments
# the kth segment is excluded from the learning sample
learn = Data_Maxine_MICHAUX[segments!=k,]
# the test sample is just the kth segment
test = Data_Maxine_MICHAUX[segments==k,]
# Fit the whole classification rule on the learning sample
image.mlog = multinom(E~.,
data=Data_Maxine_MICHAUX,trace=FALSE)
# Extract AIC for best models with k explanatory variables
AIC = image.mlog$Subsets$AIC
# Identifies column numbers for selected explanatory variables
selected = unlist(image.select$Subsets[which.min(AIC),2:6])
# Fit the model with selected axplanatory variables
image.logit = glm(E~.,family=binomial,
data=learn[,c(selected,TRUE)])
# Calculate CV'd probabilities of being in maturity group 2
cvprobas[segments==k] = predict(image.logit,newdata=test,type="response")
}
# Step 1: segmentation of the dataset in 10 segments
segments = fold(Data_Maxine_MICHAUX,k=10,cat_col="E")$".folds"
# 10-fold balanced partition of the sample
# segments is a vector giving the segment number of each item
table(Data_Maxine_MICHAUX$E,segments)
# Step 2: cycling over the segments
cvprobas = rep(0,n) # will contain the cross-validated probabilities
for (k in 1:10) {          # cycling over the 10 segments
# the kth segment is excluded from the learning sample
learn = Data_Maxine_MICHAUX[segments!=k,]
# the test sample is just the kth segment
test = Data_Maxine_MICHAUX[segments==k,]
# Fit the whole classification rule on the learning sample
image.mlog = multinom(E~.,
data=Data_Maxine_MICHAUX,trace=FALSE)
# Extract AIC for best models with k explanatory variables
AIC = image.mlog$Subsets$AIC
# Identifies column numbers for selected explanatory variables
selected = unlist(image.mlogt$Subsets[which.min(AIC),2:6])
# Fit the model with selected axplanatory variables
image.logit = glm(E~.,family=binomial,
data=learn[,c(selected,TRUE)])
# Calculate CV'd probabilities of being in maturity group 2
cvprobas[segments==k] = predict(image.logit,newdata=test,type="response")
}
# Step 1: segmentation of the dataset in 10 segments
segments = fold(Data_Maxine_MICHAUX,k=10,cat_col="E")$".folds"
# 10-fold balanced partition of the sample
# segments is a vector giving the segment number of each item
table(Data_Maxine_MICHAUX$E,segments)
# Step 2: cycling over the segments
cvprobas = rep(0,n) # will contain the cross-validated probabilities
for (k in 1:10) {          # cycling over the 10 segments
# the kth segment is excluded from the learning sample
learn = Data_Maxine_MICHAUX[segments!=k,]
# the test sample is just the kth segment
test = Data_Maxine_MICHAUX[segments==k,]
# Fit the whole classification rule on the learning sample
image.mlog = multinom(E~.,
data=Data_Maxine_MICHAUX,trace=FALSE)
# Extract AIC for best models with k explanatory variables
AIC = image.mlog$Subsets$AIC
# Identifies column numbers for selected explanatory variables
selected = unlist(image.mlog$Subsets[which.min(AIC),2:6])
# Fit the model with selected axplanatory variables
image.logit = glm(E~.,family=binomial,
data=learn[,c(selected,TRUE)])
# Calculate CV'd probabilities of being in maturity group 2
cvprobas[segments==k] = predict(image.logit,newdata=test,type="response")
}
# Step 1: segmentation of the dataset in 10 segments
segments = fold(Data_Maxine_MICHAUX,k=10,cat_col="E")$".folds"
# 10-fold balanced partition of the sample
# segments is a vector giving the segment number of each item
table(Data_Maxine_MICHAUX$E,segments)
# Step 2: cycling over the segments
cvprobas = rep(0,n) # will contain the cross-validated probabilities
for (k in 1:10) {          # cycling over the 10 segments
# the kth segment is excluded from the learning sample
learn = Data_Maxine_MICHAUX[segments!=k,]
# the test sample is just the kth segment
test = Data_Maxine_MICHAUX[segments==k,]
# Fit the whole classification rule on the learning sample
image.mlog = multinom(E~.,
data=bact,trace=FALSE)
# Extract AIC for best models with k explanatory variables
AIC = image.mlog$Subsets$AIC
# Identifies column numbers for selected explanatory variables
selected = unlist(image.mlog$Subsets[which.min(AIC),2:6])
# Fit the model with selected axplanatory variables
image.logit = glm(E~.,family=binomial,
data=learn[,c(selected,TRUE)])
# Calculate CV'd probabilities of being in maturity group 2
cvprobas[segments==k] = predict(image.logit,newdata=test,type="response")
}
# First, random permutation of sampling items
n = nrow(bact) # nb ab ds jdd
permutation = sample(1:n) # on permute tous ab manière aleat
n = nrow(Data_Maxine_MICHAUX) # nb ab ds jdd
permutation = sample(1:n) # on permute tous ab manière aleat
n = nrow(Data_Maxine_MICHAUX) # nb ab ds jdd
permutation = sample(1:n) # on permute tous ab manière aleat
# Then, split the data in 2/3 for learning, 1/3 for test
learn_ids = permutation[1:round((2/3)*n)]
learn = bact[learn_ids,-7]
n = nrow(Data_Maxine_MICHAUX) # nb ab ds jdd
permutation = sample(1:n) # on permute tous ab manière aleat
# Then, split the data in 2/3 for learning, 1/3 for test
learn_ids = permutation[1:round((2/3)*n)]
learn = bact[learn_ids,-7]
# First, random permutation of sampling items
n = nrow(Data_Maxine_MICHAUX) # nb ab ds jdd
permutation = sample(1:n) # on permute tous ab manière aleat
# Then, split the data in 2/3 for learning, 1/3 for test
learn_ids = permutation[1:round((2/3)*n)]
learn = bact[learn_ids,]
# First, random permutation of sampling items
n = nrow(Data_Maxine_MICHAUX) # nb ab ds jdd
permutation = sample(1:n) # on permute tous ab manière aleat
# Then, split the data in 2/3 for learning, 1/3 for test
learn_ids = permutation[1:round((2/3)*n)]
learn = bact[learn_ids,-7]
# First, random permutation of sampling items
n = nrow(Data_Maxine_MICHAUX) # nb ab ds jdd
permutation = sample(1:n) # on permute tous ab manière aleat
# Then, split the data in 2/3 for learning, 1/3 for test
learn_ids = permutation[1:round((2/3)*n)]
learn = bact[learn_ids,-7]
n = nrow(Data_Maxine_MICHAUX) # nb ab ds jdd
permutation = sample(1:n) # on permute tous ab manière aleat
# Then, split the data in 2/3 for learning, 1/3 for test
learn_ids = permutation[1:round((2/3)*n)]
learn = Data_Maxine_MICHAUX[learn_ids,-7]
test = Data_Maxine_MICHAUX[-learn_ids,-7]
n = nrow(Data_Maxine_MICHAUX) # nb ab ds jdd
permutation = sample(1:n) # on permute tous ab manière aleat
# Then, split the data in 2/3 for learning, 1/3 for test
learn_ids = permutation[1:round((2/3)*n)]
learn = Data_Maxine_MICHAUX[learn_ids,-7]
test = Data_Maxine_MICHAUX[-learn_ids,-7]
# Fit the whole classification rule on the learning sample
image.select = bestglm(Xy=learn,family=binomial,
method="exhaustive",IC="AIC")
AIC = image.select$Subsets$AIC
image.select = bestglm(Xy=learn,family=binomial,
method="exhaustive",IC="AIC")
image.logit = glm(E~., data=Data_Maxine_MICHAUX, family=binomial)
image.logit = glm(E~., data=bact, family=binomial)
image.logit = glm(E~StdDev, data=bact, family=binomial)
image.logit = glm(E~StdDev, data=bact, family=binomial)
image.logit = glm(E~StdDev, data=bact, family=binomial)
image.logit = glm(E~StdDev, data=bact, family=binomial)
image.logit = glm(E~StdDev, data=bact, family=binomial)
don <- read.table("http://factominer.free.fr/missMDA/ozoneNA.csv",header=TRUE,sep=",",row.names=1)
summary(don)
library(VIM)
install.packages("vim")
install.packages("VIM")
library(VIM)
library(VIM)
res <- summary(aggr(don,prop=TRUE,combined=TRUE))$combinations
res[rev(order(res[,2])),]
matrixplot(don,sortby=2)
marginplot(don[,c("T9","maxO3")])11 / 88
marginplot(don[,c("T9","maxO3")])
matrixplot(don,sortby=2)
marginplot(don[,c("T9","maxO3")])
mis.ind <- matrix("o",nrow=nrow(don),ncol=ncol(don))
mis.ind[is.na(don)]="m"
dimnames(mis.ind)=dimnames(don)
mis.ind
mis.ind;
mis.ind
resMCA <- MCA(mis.ind)
library(FactoMineR)
resMCA <- MCA(mis.ind)
plot(resMCA,invis="ind",title="MCA graph of the categories")
install.packages("norm")
library(norm)
pre <- prelim.norm(as.matrix(don)) # fait des manipulations préliminaires
thetahat <- em.norm(pre)# estime par MV
getparam.norm(pre,thetahat)# résultats⇒Variances
library(missMDA)
nb <- estim_ncpPCA(don, method.cv="Kfold")
nb <- estim_ncpPCA(don, method.cv="Kfold")
ecological <- read.csv2("~/Documents/M2/analyse senso/ecological.csv")
View(ecological)
eco<- read.csv("~/Documents/M2/analyse senso/ecological.csv")
eco<- read.table("~/Documents/M2/analyse senso/ecological.csv")
res <- summary(aggr(eco,prop=TRUE,combined=TRUE))$combinations
res[rev(order(res[,2])),]
aggr(eco,prop=TRUE,combined=TRUE)$combinations
eco<- read.csv2("~/Documents/M2/analyse senso/ecological.csv", )
aggr(eco,prop=TRUE,combined=TRUE)$combinations
res[rev(order(res[,2])),]
res[rev(order(res[,2])),]$combinations
res <- summary(aggr(eco,prop=TRUE,combined=TRUE))$combinations
res[rev(order(res[,2])),]$combinations
matrixplot(eco,sortby=2)
res <- summary(aggr(eco,prop=TRUE,combined=TRUE))$combinations
nb <- estim_ncpPCA(eco, method.cv="Kfold")
eco<-eco[,-1]
nb <- estim_ncpPCA(eco, method.cv="Kfold")
Ecolo <- read.csv2("~/Documents/M2/analyse senso/ecological.csv", header = TRUE, sep=";",dec=",")
## Delete species with only missing values for contiuous variables
ind <- which(rowSums(is.na(Ecolo[,-1])) == 6)
biome <- Ecolo[-ind,1]    ### Keep a categorical variable
Ecolo <- Ecolo[-ind,-1]   ### Select continuous variables
dim(Ecolo)
## proportion of missing values
sum(is.na(Ecolo))/(nrow(Ecolo)*ncol(Ecolo)) # 55% of missing values
## Delete species with missing values
dim(na.omit(Ecolo)) # only 72 remaining species!
#### Visualize the pattern
library(VIM)
aggr(Ecolo)
aggr(Ecolo,only.miss=TRUE,numbers=TRUE,sortVar=TRUE)
res <- summary(aggr(Ecolo,prop=TRUE,combined=TRUE))$combinations
res[rev(order(res[,2])),]
mis.ind <- matrix("o",nrow=nrow(Ecolo),ncol=ncol(Ecolo))
res
resMCA
plot(resMCA,invis="ind",title="MCA graph of the categories")
### nb <- estim_ncpPCA(Ecolo,method.cv="Kfold",nbsim=100) ### Time consuming!
res.comp <- imputePCA(Ecolo,ncp=2)
res.comp
plot(resMCA,invis="ind",title="MCA graph of the categories")
dimnames(mis.ind) <- dimnames(Ecolo)
dimnames(mis.ind)
res.comp
AB_NYC_2019 <- read.csv("~/Documents/M2/Cours_Agro_J1_Programmation_R/archive/Projet/AB_NYC_2019.csv")
View(AB_NYC_2019)
setwd("~/Documents/M2/Cours_Agro_J1_Programmation_R/archive/Projet")
testdata <- fread("./AB_NYC_2019.csv", header=T,stringsAsFactors = T)
library(data.table)
setwd("~/Documents/M2/Cours_Agro_J1_Programmation_R/archive/Projet")
testdata <- fread("./AB_NYC_2019.csv", header=T,stringsAsFactors = T)
# Define server logic required to draw a histogram
shinyServer(function(input, output, session) {
output$intro <- renderText({
paste("Vous etes a la recherche d'un logement a New-York ?")
})
observeEvent(input$run2, {
testdata[testdata$neighbourhood_group == input$quartier]
testdata[testdata$room_type == input$logement]
testdata[testdata$price >= input$price[1] & testdata$price <= input$price[2],]
})
# filtered_data <- reactive(input$run2, {
#     testdata[testdata$neighbourhood_group == input$quartier]
#     testdata[testdata$room_type == input$logement]
#     testdata[testdata$price >= input$price[1] & testdata$price <= input$price[2],]
# })
pal <- colorFactor("viridis", testdata$room_type)
output$map <- renderLeaflet({
leaflet() %>%
addCircles(lng = ~longitude, lat = ~latitude) %>%
addCircleMarkers(data = testdata[1:100], lat =  ~latitude, lng =~longitude,
radius = 3,
color = ~pal(room_type),
stroke = FALSE, fillOpacity = 0.8) %>%
addLegend(position = "topright",
pal=pal,
values=testdata$room_type,
opacity=1,
title = "Type de logement",
group = "circles") #%>%
# addEasyButton(easyButton(
#     icon="fa-crosshairs", title="ME",
#     onClick=JS("function(btn, map){ map.locate({setView: true}); }")))
})
observe({
# updateSelectizeInput(session, inputId = "quartier",
#                   label = "Selectionnez le quartier",
#                   choices = testdata$neighbourhood_group)
#
# updateSelectizeInput(session, inputId = "logement",
#                      label = "Selectionnez le type de logement",
#                      choices = testdata$room_type)
#
# updateSliderInput(session, inputId = "price",
#                     label = "Selectionnez le prix desire pour une nuit entre : ",
#                     min = min(input$price[1]),
#                     max = max(input$price[2]),
#                     value = filter(testdata[testdata$price >= min & testdata$price <= max]))
leafletProxy("map", data = testdata) %>%
clearShapes() %>%
addCircles(lng = ~longitude, lat = ~latitude %>%
addCircleMarkers(data = testdata, lat =  ~latitude, lng =~longitude,
radius = 3,
color = ~pal(room_type),
stroke = FALSE, fillOpacity = 0.8) %>%
addLegend(position = "topright",
pal=pal,
values=testdata$room_type,
opacity=1,
title = "Type de logement",
group = "circles")
)
})
})
shiny::runApp('~/Documents/M2/Cours_Agro_J1_Programmation_R/carte')
